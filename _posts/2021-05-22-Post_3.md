---
layout: post
title: Blog Post 3 - Tensorflow
---

In this tutorial, I will be showing you how to use Tensorflow to detect fake news

## Loading Data 

The dataset that I'll be using is from the following article:
- Ahmed H, Traore I, Saad S. (2017) “Detection of Online Fake News Using N-Gram Analysis and Machine Learning Techniques. In: Traore I., Woungang I., Awad A. (eds) Intelligent, Secure, and Dependable Systems in Distributed and Cloud Environments. ISDDC 2017. Lecture Notes in Computer Science, vol 10618. Springer, Cham (pp. 127-138).

This dataset is available to the public on Kaggle, but I will be accessing it from my PIC16b professor Phil Chodrow's website. It has 22449 rows and 4 columns.


```python
import pandas as pd 
import numpy as np 

train_url = "https://github.com/PhilChodrow/PIC16b/blob/master/datasets/fake_news_train.csv?raw=true"
df = pd.read_csv(train_url)
df.head()
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Unnamed: 0</th>
      <th>title</th>
      <th>text</th>
      <th>fake</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>17366</td>
      <td>Merkel: Strong result for Austria's FPO 'big c...</td>
      <td>German Chancellor Angela Merkel said on Monday...</td>
      <td>0</td>
    </tr>
    <tr>
      <th>1</th>
      <td>5634</td>
      <td>Trump says Pence will lead voter fraud panel</td>
      <td>WEST PALM BEACH, Fla.President Donald Trump sa...</td>
      <td>0</td>
    </tr>
    <tr>
      <th>2</th>
      <td>17487</td>
      <td>JUST IN: SUSPECTED LEAKER and “Close Confidant...</td>
      <td>On December 5, 2017, Circa s Sara Carter warne...</td>
      <td>1</td>
    </tr>
    <tr>
      <th>3</th>
      <td>12217</td>
      <td>Thyssenkrupp has offered help to Argentina ove...</td>
      <td>Germany s Thyssenkrupp, has offered assistance...</td>
      <td>0</td>
    </tr>
    <tr>
      <th>4</th>
      <td>5535</td>
      <td>Trump say appeals court decision on travel ban...</td>
      <td>President Donald Trump on Thursday called the ...</td>
      <td>0</td>
    </tr>
  </tbody>
</table>
</div>



Next I'll create a function `make_dataset` that does three things:
1. Removes the stopwords from `title` and `text`. Stopwords are words that don't hold much useful information such as "the" and "a".  Removing such words saves us both memory and training time.  

2. Constructs a Tensorflow dataset from the Pandas Dataframe. This is not required for to use a Tensorflow model, but it makes it easier to stay organized when writing data pipelines. 


```python
import tensorflow as tf 
import nltk
nltk.download('stopwords')
from nltk.corpus import stopwords

# the main make_dataset function 
def make_dataset(df):
    '''
    Transforms df to a Tensorflow dataset 
    '''
    #remove the stopwords from title and text 
    stop = stopwords.words('english')
    df['text'] = df['text'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)]))
    df['title'] = df['title'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)]))

    #creating a tensorflow dataset 
    data = tf.data.Dataset.from_tensor_slices(
        (
            {
                'title' : df[['title']], 
                'text' : df[['text']]
            }, 
            {
                'fake' : df[["fake"]]
            }
        )
    )
    data = data.batch(100)
    return data 

#apply the make_dataset function 
data = make_dataset(df)
```

    [nltk_data] Downloading package stopwords to /root/nltk_data...
    [nltk_data]   Package stopwords is already up-to-date!
    
<div class="got-help">
I originally did a separate helper function to remove the stopwords, but I took the advice from my classmates to use lambda function instead. 
</div>


Note when we create a Tensorflow dataset, we have somewhat of a funny syntax. The Tensorflow dataset takes in the input and output variables in a tuple, i.e. (input, output). When we have multiple input variables, we use a dictionary to point Tensorflow to the correct columns. The resulting Tensorflow dataset would have `(title, text)` as the input and `fake` as the output.

Lastly, I'll shuffle the dataset and take the first 80% as traning and the last 20% as validation. Note the length of the dataset refers to the # of batches. So to get the # of rows, we need to multiply by 100. 


```python
# shuffle the data 
data = data.shuffle(buffer_size = len(data))

#make 80% training set and 20% validation set 
train_size = int(0.8*len(data))
val_size   = int(0.2*len(data))

train = data.take(train_size)
val   = data.skip(train_size).take(val_size) 

#check the lengths of train and val 
len(train), len(val)
```




    (180, 45)



## Vectorization

When we have textual data, we need to turn it into vectors/arrays so that the model can understand it. Before I get into the actual vectorization part, I'll first write a `standardization` function that does the following: 
1. Lowercase all letters
2. Removes all punctuations


```python
import re
import string

def standardization(input_data):
    lowercase = tf.strings.lower(input_data) #lowercase 
    no_punctuation = tf.strings.regex_replace(lowercase, #remove punctuations
                                  '[%s]' % re.escape(string.punctuation),'')
    return no_punctuation 
```


There are multiple methods to vectorize texts, and the method I'll be using today is the *frequency* method. Intuitively, this is equivalent to replacing each word with its relative frequency in the whole dataset. Say the sentence
> Samoyed is the best dog breed.

is transformed into the following vector: 

> [1000, 20, 40, 87, 350, 890]

This means "samoyed" is the 1000th most common word in the dataset, "is" is the 20th most common, so on and so forth. (In reality, "is" and "the" wouldn't even make it to this step because they would have been removed by the standardization). 






```python
from tensorflow.keras.layers.experimental.preprocessing import TextVectorization

vectorize_layer = TextVectorization(
    standardize=standardization,
    max_tokens=2000, # only consider 2000 words 
    output_mode='int',
    output_sequence_length=500 #restricts the size of each vector to 500 
    ) 

#adapt the vectorize_layer on both title and text 
vectorize_layer.adapt(train.map(lambda x, y: x['title'] + x['text'])) 
```

## Modeling

Keras Functional API is extremely useful when we have multiple inputs, which in this case we have `title` and `text`. 

For this demonstration, I'll be creating three models using the Keras Functional API.
1. In the first model, I'll be only using `title`.
2. In the second model, I'll be only using `text`.
3. In the third model, I'll be using both `title` and `text`. 


There are two things I need to do before constructing the model. First, I'll specify the two inputs.


```python
from tensorflow.keras import layers
from tensorflow.keras import losses
from tensorflow import keras

#specifying the two inputs
title_input = keras.Input(
    shape = (1,), 
    name = "title",
    dtype = "string"
)

text_input = keras.Input(
    shape = (1,), 
    name = "text",
    dtype = "string"
)
```

Next I'll create a shared embedding layer for both title and text inputs. I'll talk more about what embedding does later. 


```python
#creating a shared embedding layer for both title and text input 
embed_layer = layers.Embedding(2000, 10, name = "embedding")
```

### Model 1 - Title Only 

Now I'll write the pipeline for the first model. If this is your first time seeing Tensorflow, you might be wondering what all these layers mean. 

The Embedding layer tries to project the words into a vector space. Ideally, words with similar meanings should be close together, vice versa. Also, pairs with the same relationship (eg. Good-Bad, Bright-Dark) should be in the same direction.

The Dropout layer and the Pooling layer both aim to reduce overfitting. Finally, the Dense layer is just the most basic type of hidden layer in neural network. It assigns a weight to each of the node in the previous layer and feeds the resulting linear combination into a nonlinear function. 


```python
title_features = vectorize_layer(title_input) 
title_features = embed_layer(title_features)
title_features = layers.Dropout(0.2)(title_features)
title_features = layers.GlobalAveragePooling1D()(title_features)
title_features = layers.Dropout(0.2)(title_features)
title_features = layers.Dense(32, activation='relu')(title_features)

#I'm switching the name for the output layer 
#because I want the concatenation to happen at the previous layer. 
#Also note that the output layer has to have the same name as the output variable 
x = layers.Dense(2, name = 'fake')(title_features)
```


```python
#create the model 
model1 = keras.Model(
    inputs = title_input,
    outputs = x
)

#check out the model summary 
model1.summary()
```

    Model: "model"
    _________________________________________________________________
    Layer (type)                 Output Shape              Param #   
    =================================================================
    title (InputLayer)           [(None, 1)]               0         
    _________________________________________________________________
    text_vectorization (TextVect (None, 500)               0         
    _________________________________________________________________
    embedding (Embedding)        (None, 500, 10)           20000     
    _________________________________________________________________
    dropout (Dropout)            (None, 500, 10)           0         
    _________________________________________________________________
    global_average_pooling1d (Gl (None, 10)                0         
    _________________________________________________________________
    dropout_1 (Dropout)          (None, 10)                0         
    _________________________________________________________________
    dense (Dense)                (None, 32)                352       
    _________________________________________________________________
    fake (Dense)                 (None, 2)                 66        
    =================================================================
    Total params: 20,418
    Trainable params: 20,418
    Non-trainable params: 0
    _________________________________________________________________


Next we'll compile and fit the model. Since this part of the code would be the same for all three models, I'll create a function `compile_and_fit` that handles the compilation and training process.

In addition, I'll create a `plot_history` function that plots the training hostry. 


```python
def compile_and_fit(model, epochs):
    '''
    Compiles and trains a model for the specified number of epochs and 
    Returns the training history 
    ''' 
    model.compile(optimizer = "adam",
              loss = losses.SparseCategoricalCrossentropy(from_logits=True),
              metrics=['accuracy']
              ) 
    history = model.fit(train, 
                    validation_data=val,
                    epochs = epochs, 
                    verbose = False)
    return history


from matplotlib import pyplot as plt

def plot_history(history):
    '''
    Plots the training history 
    '''
    plt.plot(history.history["accuracy"], label = "training")
    plt.plot(history.history["val_accuracy"], label = "validation")
    plt.plot([0,20], [0.97,0.97], color = 'red', label = 'y = 0.97')
    plt.gca().set(xlabel = "epoch", ylabel = "accuracy")
    plt.legend()
```
<div class="got-help">
I took some inspiration from my classmates to write the compile_and_fit function. But I decided to keep model declaration outside of this function because I think it's more readable this way.  
</div>

Finally let's apply the two functions on Model 1. 


```python
history1 = compile_and_fit(model1, 20)

plot_history(history1)
```


![output_25_0.png](/images/output_25_0.png)

<div class="gave-help">
I got some compliments for including the y=0.97 line in my plot as it makes it easier for readers to compare and contrast. 
</div>


Both the training and validation accuracy reached 0.97 after around 7 epochs. And they steadily converge to 1.0 as the number of epochs increase. This means that the model is able to identify fake news very well using JUST the title. 

### Model 2 - Text Only

Next I'll write the pipelines for the second model, which uses `text` only. The layers that I'll be using are the same as before, so I'll skip the explanation this time. 


```python
text_features = vectorize_layer(text_input)
text_features = embed_layer(text_features)
text_features = layers.Dropout(0.2)(text_features)
text_features = layers.GlobalAveragePooling1D()(text_features)
text_features = layers.Dropout(0.2)(text_features)
text_features = layers.Dense(32, activation='relu')(text_features)  

#I'm switching the name for the output layer 
#because I want the concatenation to happen at the previous layer. 
#Also note that the output layer has to have the same name as the output variable 
y = layers.Dense(2, name = 'fake')(text_features) 
```


```python
#create the model 
model2 = keras.Model(
    inputs = text_input,
    outputs = y
)

#check out the model summary 
model2.summary()
```

    Model: "model_1"
    _________________________________________________________________
    Layer (type)                 Output Shape              Param #   
    =================================================================
    text (InputLayer)            [(None, 1)]               0         
    _________________________________________________________________
    text_vectorization (TextVect (None, 500)               0         
    _________________________________________________________________
    embedding (Embedding)        (None, 500, 10)           20000     
    _________________________________________________________________
    dropout_2 (Dropout)          (None, 500, 10)           0         
    _________________________________________________________________
    global_average_pooling1d_1 ( (None, 10)                0         
    _________________________________________________________________
    dropout_3 (Dropout)          (None, 10)                0         
    _________________________________________________________________
    dense_1 (Dense)              (None, 32)                352       
    _________________________________________________________________
    fake (Dense)                 (None, 2)                 66        
    =================================================================
    Total params: 20,418
    Trainable params: 20,418
    Non-trainable params: 0
    _________________________________________________________________



```python
#compiles and fits model2
history2 = compile_and_fit(model2, 20)

#plots the history
plot_history(history2)
```

    /usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/functional.py:595: UserWarning: Input dict contained keys ['title'] which did not match any model input. They will be ignored by the model.
      [n for n in tensors.keys() if n not in ref_input_names])



![output_30_1.png](/images/output_30_1.png)


As shown, both the training and validation accuracy reached 0.97 after around 8 epochs. This is a somewhat surprising result since we generally expect the text of a news article to contain more information than its title. 

### Model 3 - Title and Text

Finally, we'll build a model that uses both the title and the text as its inputs. 

Recall I've intentionally changed the name for the last Dense layer, this is because I want the concatenation to happen at the second to last layer. Now we just need to concatenate and add another Dense layer after the concatenated layer. 


```python
#concatenate the Model 1 and Model 2
main = layers.concatenate([title_features, text_features], axis = 1)
output = layers.Dense(2, name = "fake")(main)
```


```python
model3 = keras.Model(
    inputs = [title_input, text_input],
    outputs = output
)
```

For concatenated models, it's often helpful to draw a flow chart. Fortunately, `keras.utils` comes with a handy function called `plot_model` that allows us to visualize our model. 


```python
keras.utils.plot_model(model3)
```




![output_36_0.png](/images/output_36_0.png)



Now let's compile and fit our model. 


```python
#compile and fit model3
history3 = compile_and_fit(model3, 20)

#plot the training history 
plot_history(history3)
```


![output_38_0.png](/images/output_38_0.png)


With both `title` and `text`, both the training and validation accuracy reached 0.98 after only ONE epoch. This is significantly better than the previous two models. The next thing that we'll do is to test this model on unseen data.  

## Model Evaluation

Now it's time to test our model on unseen data. The testing data that I'll be using is from the following source:


```python
test_url = "https://github.com/PhilChodrow/PIC16b/blob/master/datasets/fake_news_test.csv?raw=true"
```

First let's convert it into Tensorflow dataset using the `make_dataset` function that we defined before.


```python
test = pd.read_csv(test_url) #read in test_url to a dataframe 
test = make_dataset(test) #convert to a Tensorflow dataset
```


```python
model3.evaluate(test)
```

    225/225 [==============================] - 2s 11ms/step - loss: 0.0312 - accuracy: 0.9911





    [0.03121376410126686, 0.9910908937454224]



As shown, our model using both `text` and `title` is able to correctly identify 99.11% of the unseen data. 

## Embedding Visualization

Recall the embedding layer tries to project words into a space, so that 1) words sharing the same meaning are close to each other and 2) pairs of words that have the same relationship are in the same direction. Visualizing the results of the embedding layer can often reveal some interesting patterns. 

First let's get the weights from our embedding layer. Note the shape of the `weights` matrix is 2000 by 10, where 2000 is the maximum # of words that we specifid for the vectorization layer and 10 is the dimension of the space that the words are being projected to. 


```python
weights = model3.get_layer('embedding').get_weights()[0] # get the weights from the embedding layer
weights
```




    array([[-1.8590747e-03, -2.1421206e-03, -1.3934528e-03, ...,
            -7.3854654e-04,  1.9814048e-03, -4.4971574e-04],
           [ 9.7373903e-02, -9.7989783e-02, -7.5589873e-02, ...,
            -4.9663927e-02, -7.4048936e-02,  2.7484816e-01],
           [-1.6077874e+00,  1.6884406e+00,  1.6172780e+00, ...,
             1.5962821e+00,  1.6757476e+00,  4.8073912e-01],
           ...,
           [-7.8609139e-01,  8.1808168e-01,  8.2799214e-01, ...,
             7.9030246e-01,  7.6916045e-01, -4.1779891e-01],
           [-4.8763204e-01,  4.7676501e-01,  3.4226328e-01, ...,
             5.0073820e-01,  5.9152377e-01, -3.8699916e-01],
           [-4.6298221e-02,  6.0192514e-02,  6.8499260e-02, ...,
            -3.7052257e-03, -4.0591624e-02,  2.1827535e-01]], dtype=float32)



Of course, our human brains can't visualize a space that's 10-dimensional (it can't really go beyond 3). So we are going to reduce the dimensions to 2 using PCA.


```python
from sklearn.decomposition import PCA
pca = PCA(n_components=2)
weights = pca.fit_transform(weights)
```

Next, I'll create a dataframe that contains each word and its position in a 2-D space.


```python
vocab = vectorize_layer.get_vocabulary()  # get the vocabulary from our data prep for later

# make a dataframe 
embedding_df = pd.DataFrame({
    'word' : vocab, 
    'x0'   : weights[:,0],
    'x1'   : weights[:,1]
})
embedding_df.head()
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>word</th>
      <th>x0</th>
      <th>x1</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td></td>
      <td>0.063660</td>
      <td>0.049760</td>
    </tr>
    <tr>
      <th>1</th>
      <td>[UNK]</td>
      <td>-0.198613</td>
      <td>0.292393</td>
    </tr>
    <tr>
      <th>2</th>
      <td>said</td>
      <td>4.869554</td>
      <td>1.141396</td>
    </tr>
    <tr>
      <th>3</th>
      <td>trump</td>
      <td>-0.373748</td>
      <td>-0.793620</td>
    </tr>
    <tr>
      <th>4</th>
      <td>the</td>
      <td>0.543729</td>
      <td>3.166853</td>
    </tr>
  </tbody>
</table>
</div>



Finally I'll make a scatter plot using plotly.


```python
import plotly.express as px

fig = px.scatter(embedding_df,
           x= 'x0',
           y = 'x1',
           size = list(np.ones(len(embedding_df))),
           size_max = 2,
           hover_name = "word"
           )

fig.show()
```

{% include blog3_scatter.html %}

On the bottom left, we can see '21st', 'century', and '21wire'. Out of curiosity, I did some googling. These words seem to be referring to a website called 21st Century Wire, sometimes also called 21Wire. According to https://mediabiasfactcheck.com/, "21st Century Wire is an alternative news website that rejects the consensus of science regarding climate change and also promotes the conspiracy that mainstream media is publishing fake news". David E. Van Zandt, former Dean of Northwestern University, calls it "a conspiracy and fake news website with an extreme right bias. Additionally, their headlines are sensationalist and largely misleading". 

<div class="gave-help">
I also got some compliments here for doing this mini-research regarding 21st Century Wire. 
</div>


