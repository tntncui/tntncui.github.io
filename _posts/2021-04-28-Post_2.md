---
layout: post
title: Blog Post 2
---

In this blog post, I will be showing you how to build a simple spectral clustering algorithm using NumPy and our dear friend Linear Algebra. 


## Introduction

So what is spectral clustering? It is an important tool for identifying meaningful parts of data sets with complex structure. To start, let's look at an example where we *don't* need spectral clustering. 


```python
import numpy as np
from sklearn import datasets
from matplotlib import pyplot as plt
```


```python
n = 200
np.random.seed(1111)
X, y = datasets.make_blobs(n_samples=n, shuffle=True, random_state=None, centers = 2, cluster_std = 2.0)
plt.scatter(X[:,0], X[:,1])
```

![png](/images/output_2_1.png)


*Clustering* refers to the task of this data set into the two natural "blobs." K-means is a very common way to achieve this task, which has good performance on circular-ish blobs like these: 


```python
from sklearn.cluster import KMeans
km = KMeans(n_clusters = 2)
km.fit(X)

plt.scatter(X[:,0], X[:,1], c = km.predict(X))
```

![png](/images/output_4_1.png)


### Harder Clustering

That was all well and good, but what if our data is "shaped weird"? 


```python
np.random.seed(1234)
n = 200
X, y = datasets.make_moons(n_samples=n, shuffle=True, noise=0.05, random_state=None)
plt.scatter(X[:,0], X[:,1])
```
![png](/images/output_6_1.png)


We can still make out two meaningful clusters in the data, but now they aren't blobs but crescents. As before, the Euclidean coordinates of the data points are contained in the matrix `X`, while the labels of each point are contained in `y`. Now k-means won't work so well, because k-means is, by design, looking for circular clusters. 


```python
km = KMeans(n_clusters = 2)
km.fit(X)
plt.scatter(X[:,0], X[:,1], c = km.predict(X))
```

![png](/images/output_8_1.png)


Whoops! That's not right! This is an example where spectral clustering might be helpful. 

## Part A

Our first task is to construct a *similarity matrix* $$\mathbf{A}$$. $$\mathbf{A}$$ is a 2D array with the shape `(n, n)` (recall that `n` is the number of data points). 

When constructing the similarity matrix, we use a parameter `epsilon`. Entry `A[i,j]` should be equal to `1` if `X[i]` (the coordinates of data point `i`) is within distance `epsilon` of `X[j]` (the coordinates of data point `j`). Intuitively, we are saying that we want `A[i,j]` to be 1 if `X[i]` is close to `X[j]` and 0 otherwise. 

In addition, the diagonal of A should be all zeros. For this part, we use `epsilon = 0.4`. 


```python
from sklearn.metrics import pairwise_distances #this function automatically calculates the pairwise distances and stores it in a matrix 
B = pairwise_distances(X)
epsilon = 0.4 
A = (B < epsilon).astype(int) #check if each entry is within the designated epsilon
np.fill_diagonal(A, 0) #fill the diagonal with 0 
A
```




    array([[0, 0, 0, ..., 0, 0, 0],
           [0, 0, 0, ..., 0, 0, 0],
           [0, 0, 0, ..., 0, 1, 0],
           ...,
           [0, 0, 0, ..., 0, 1, 1],
           [0, 0, 1, ..., 1, 0, 1],
           [0, 0, 0, ..., 1, 1, 0]])



## Part B

The matrix $$\mathbf{A}$$ now contains information about which points are near (within distance `epsilon`) which other points. We now pose the task of clustering the data points in `X` as the task of partitioning the rows and columns of $$\mathbf{A}$$. 

Let's get some notations cleared out of the way. Let $$d_i = \sum_{j = 1}^n a_{ij}$$ be the $$i$$th row-sum of $$\mathbf{A}$$. Lev $$C_0$$ refer to cluster 0, and $$C_1$$ refer to cluster 1. The label vector `y` contains information about which group each point belongs to; precisely point i belongs to group 0 if `y[i]` is 0, and group 1 otherwise. 


The *binary norm cut objective* of a matrix A is the function 

$$N_{\mathbf{A}}(C_0, C_1)\equiv \mathbf{cut}(C_0, C_1)\left(\frac{1}{\mathbf{vol}(C_0)} + \frac{1}{\mathbf{vol}(C_1)}\right)\;.$$

In this expression, 
- $$\mathbf{cut}(C_0, C_1) \equiv \sum_{i \in C_0, j \in C_1} a_{ij}$$ is the *cut* of the clusters $$C_0$$ and $$C_1$$. 
- $$\mathbf{vol}(C_0) \equiv \sum_{i \in C_0}d_i$$, where $$d_i = \sum_{j = 1}^n a_{ij}$$ is the *degree* of row $i$ (the total number of all other rows related to row $$i$$ through A). The *volume* of cluster $$C_0$$ is a measure of the size of the cluster. 

A pair of clusters is considered a good partition when the binary norm cut is small. To see why, let's look at each of the two factors in this objective function separately. 


#### B.1 The Cut Term

First, the cut term is the number of nonzero entries in A that relate points in group 0 to group 1. Saying that this term should be small is the same as saying that points in group 0 shouldn't usually be very close to points in group 1. 

Now let's construct a function called `cut(A,y)` to compute the cut term by summing up the entries `A[i,j]` for each pair of points `(i,j)` in different clusters. 

```python
def cut(A,y):
    res = 0
    for i in range(len(A)): #loop through each entry possible combination 
        for j in range(len(A)):
            if y[i] != y[j]: 
                res += A[i,j] #update res iff the two points are in different clusters
    return res
cut(A,y) #implement the function on the true label vector
```




    26



Now let's compute the cut objective for the true label vector `y`. Then, generate a random vector of random labels of length `n`, with each label equal to either 0 or 1. Check the cut objective for the random labels. Note the cut objective for the true labels is *much* smaller than the cut objective for the random labels. 

This shows that this part of the cut objective indeed favors the true clusters over the random ones. 


```python
test = np.random.randint(0,2,size = n) #generate a vector of random labels of size n with each label equal to either 0 or 1
cut(A, test) 
```




    2232



#### B.2 The Volume Term 

Now take a look at the second factor in the norm cut objective, which is the volume term. As mentioned above, the *volume* of cluster $$C_0$$ is a measure of how "big" cluster $$C_0$$ is. If we choose cluster $$C_0$$ to be small, then $$\mathbf{vol}(C_0)$$ will be small and $$\frac{1}{\mathbf{vol}(C_0)}$$ will be large, leading to an undesirable higher objective value. 


Now let's write a function `vols(A,y)` that returns `vol(C0)` and `vol(C1)` as a tuple. 


```python
def vols(A,y):
    return np.sum(A[y==0], axis = None), np.sum(A[y==1], axis = None)
vols(A,y)
```




    (2299, 2217)

Synthesizing, the binary normcut objective asks us to find clusters `C0` and `C1` such that:

1. There are relatively few entries of A that join `C0` and `C1`
2. Neither `C0` and `C1` are too small. 


```python
def normcut(A,y):
    v0, v1 = vols(A,y)
    return cut(A,y)*(1/v0 + 1/v1)
normcut(A,y)    
```




    0.02303682466323045



Now, let's compare the normcut objective using both the true labels `y` and the fake labels we generated above. Please note that the normcut for the true labels is much smaller than that of the fake labels. This is again a sign that the binary norm cut objective favors the true clusters. 


```python
normcut(A,test)
```




    1.991672673470162



## Part C

We have now defined a normalized cut objective which takes small values when the input. One approach to clustering is to try to find a cluster vector `y` such that `normcut(A,y)` is minimal. However, this is an NP-hard combinatorial optimization problem, which means that may not be possible to find the best clustering in practical time, even for relatively small data sets. We need a math trick! 

Here's the trick: define a new vector $$\mathbf{z} \in \mathbb{R}^n$$ such that: 

$$
z_i = 
\begin{cases}
    \frac{1}{\mathbf{vol}(C_0)} &\quad \text{if } y_i = 0 \\ 
    -\frac{1}{\mathbf{vol}(C_1)} &\quad \text{if } y_i = 1 \\ 
\end{cases}
$$


Note that the signs of  the elements of $\mathbf{z}$ contain all the information from $$\mathbf{y}$$: if $$i$$ is in cluster $$C_0$$, then $$y_i = 0$$ and $$z_i > 0$$. 

How do we connect this back to the normcut function we defined earlier? Well, check out the following equation. 

$$\mathbf{N}_{\mathbf{A}}(C_0, C_1) = 2\frac{\mathbf{z}^T (\mathbf{D} - \mathbf{A})\mathbf{z}}{\mathbf{z}^T\mathbf{D}\mathbf{z}}\;,$$

where $$\mathbf{D}$$ is the diagonal matrix with nonzero entries $$d_{ii} = d_i$$, and  where $$d_i = \sum_{j = 1}^n a_i$$ is the degree (row-sum) from before.  

 
First, let's Write a function called `transform(A,y)` to compute the appropriate $$\mathbf{z}$$ vector given `A` and `y`, using the formula above. 

```python
def transform(A,y):
	z = np.zeros(len(y)) #initialize z to be the same length as y 
    v0, v1 = vols(A,y)
    z[y == 0] = 1/v0
    z[y == 1] = 1/v1
    return z
```


Next let's check $$\mathbf{z}^T\mathbf{D}\mathbb{1} = 0$$, where $$\mathbb{1}$$ is the vector of `n` ones (i.e. `np.ones(n)`). Here we are using the `isclose(x,y)` function, which will return True if and only if the given x and y are equal to each other. 

```python
z = transform(A,y)
D = np.diag(sum(A)) #put the row sums on the diagonal of D
D
```




    array([[15.,  0.,  0., ...,  0.,  0.,  0.],
           [ 0., 25.,  0., ...,  0.,  0.,  0.],
           [ 0.,  0., 24., ...,  0.,  0.,  0.],
           ...,
           [ 0.,  0.,  0., ..., 26.,  0.,  0.],
           [ 0.,  0.,  0., ...,  0., 28.,  0.],
           [ 0.,  0.,  0., ...,  0.,  0., 26.]])




```python
np.isclose(z@D@np.ones(n), 0) #check if z@D@I is indeed 0 
```




    True


Finally, let's check the equation above that relates the matrix product to the normcut objective. Again, we'll be using `np.isclose()` function. 


```python
np.isclose(2*(z@(D-A)@z)/(z@D@z), normcut(A,y)) #check if lhs equals to the rhs
```




    True



## Part D

In the last part, we saw that the problem of minimizing the normcut objective is mathematically related to the problem of minimizing the function 

$$ R_\mathbf{A}(\mathbf{z})\equiv \frac{\mathbf{z}^T (\mathbf{D} - \mathbf{A})\mathbf{z}}{\mathbf{z}^T\mathbf{D}\mathbf{z}} $$

subject to the condition $$\mathbf{z}^T\mathbf{D}\mathbb{1} = 0$$. It's actually possible to bake this condition into the optimization, by substituting for $$\mathbf{z}$$ the orthogonal complement of $$\mathbf{z}$$ relative to $$\mathbf{D}\mathbf{1}$$. 

Then we'll use the `minimize` function from `scipy.optimize` to minimize the function `orth_obj` with respect to $$\mathbf{z}$$. 


```python
def orth(u, v):
    return (u @ v) / (v @ v)*v

e = np.ones(n) 

d = D @ e

def orth_obj(z):
    z_o = z - orth(z, d)
    return (z_o @ (D - A) @ z_o)/(z_o @ D @ z_o)
```


```python
from scipy.optimize import minimize
z_ = minimize(orth_obj, x0=np.ones(n))
```
 

## Part E

Recall that, by design, only the sign of `z_min[i]` actually contains information about the cluster label of data point `i`. 

Now let's plot the original data, using one color for points such that `z_min[i] < 0` and another color for points such that `z_min[i] >= 0`. It seems like we are doing a pretty good job! 


```python
z_min = z_.x
preds = np.zeros(len(z_min)) #initialize all entries in preds as zeros
preds[z_min < 0] = 1  #substituting entries where z_min is smaller than 0 
plt.scatter(X[:,0], X[:,1], c = preds)
```

![png](/images/output_31_1.png)


## Part F

Explicitly optimizing the orthogonal objective is  *way* too slow to be practical. If spectral clustering required that we do this each time, no one would use it. 

The reason that spectral clustering actually matters, and indeed the reason that spectral clustering is called *spectral* clustering, is that we can actually solve the problem from Part E using eigenvalues and eigenvectors of matrices. 

Recall that what we would like to do is minimize the function 

$$ R_\mathbf{A}(\mathbf{z})\equiv \frac{\mathbf{z}^T (\mathbf{D} - \mathbf{A})\mathbf{z}}{\mathbf{z}^T\mathbf{D}\mathbf{z}} $$

with respect to $$\mathbf{z}$$, subject to the condition $$\mathbf{z}^T\mathbf{D}\mathbb{1} = 0$$. 

The Rayleigh-Ritz Theorem states that the minimizing $$\mathbf{z}$$ must be the solution with smallest eigenvalue of the generalized eigenvalue problem 

$$ (\mathbf{D} - \mathbf{A}) \mathbf{z} = \lambda \mathbf{D}\mathbf{z}\;, \quad \mathbf{z}^T\mathbf{D}\mathbb{1} = 0$$

which is equivalent to the standard eigenvalue problem 

$$ \mathbf{D}^{-1}(\mathbf{D} - \mathbf{A}) \mathbf{z} = \lambda \mathbf{z}\;, \quad \mathbf{z}^T\mathbb{1} = 0\;.$$

Why is this helpful? Well, $$\mathbb{1}$$ is actually the eigenvector with smallest eigenvalue of the matrix $$\mathbf{D}^{-1}(\mathbf{D} - \mathbf{A})$$. 

> So, the vector $$\mathbf{z}$$ that we want must be the eigenvector with  the *second*-smallest eigenvalue. 

Construct the matrix $$\mathbf{L} = \mathbf{D}^{-1}(\mathbf{D} - \mathbf{A})$$, which is often called the *Laplacian* matrix of the similarity matrix $$\mathbf{A}$$. Find the eigenvector corresponding to its second-smallest eigenvalue, and call it `z_eig`. Then, plot the data again, using the sign of `z_eig` as the color. How did we do? 


```python
#construct the Laplacian matrix 
L = np.linalg.inv(D)@(D-A) 
```


```python
#calculate the eigenvalues and eigenvectors
Lam, U = np.linalg.eig(L)

#sort the eigenvalues and eigenvectors 
ix = Lam.argsort()
Lam, U = Lam[ix], U[:,ix]

#let z_eig be the eigenvector corresponding to the 2nd smallest eigenvalue
z_eig = U[:,1]
```


```python
preds_eig = np.zeros(len(z_eig)) #initialize all entries in preds_eig as zeros
preds_eig[z_eig < 0] = 1  #substituting entries where z_eig is smaller than 0 
plt.scatter(X[:,0], X[:,1], c = preds_eig)
```

![png](/images/output_35_1.png)


In fact, `z_eig` should be proportional to `z_min`, although this won't be exact because minimization has limited precision by default. 

## Part G

Now let's synthesize our results from the previous parts. In particular, we'll write a function called `spectral_clustering(X, epsilon)` which takes in the input data `X` (in the same format as Part A) and the distance threshold `epsilon` and performs spectral clustering, returning an array of binary labels indicating whether data point i is in group 0  or group 1.


```python
from sklearn.metrics import pairwise_distances 

def spectral_clustering(X,epsilon):
    '''
    Arguments:
    	X = A input data matrix 
    	epsilon = A scalar that measures how close you want the two clusters to be 
    
    Return:
    	A vector of binary labels 
    '''
    #construct similarity matrix A
    B = pairwise_distances(X)
    A = (B < epsilon).astype(int) #check if each entry is within the designated epsilon
    np.fill_diagonal(A, 0) #fill the diagonal with 0 
    
    #construct the Laplacian matrix
    L = np.linalg.inv(np.diag(sum(A)))@(np.diag(sum(A))-A) 
    
    #Compute the eigenvector with second-smallest eigenvalue of the Laplacian matrix
    Lam, U = np.linalg.eig(L)

    #sort the eigenvalues and eigenvectors 
    ix = Lam.argsort()
    Lam, U = Lam[ix], U[:,ix]

    #let z_eig be the eigenvector corresponding to the 2nd smallest eigenvalue
    z_eig = U[:,1]
    
    #form the labels based on the signs of z_eig 
    preds_eig = np.zeros(len(z_eig)) #initialize all entries in preds_eig as zeros
    preds_eig[z_eig < 0] = 1
    
    return preds_eig
```


```python
#plot the result, should be the same to the previous graph 
preds_eig = spectral_clustering(X,epsilon = 0.4)
plt.scatter(X[:,0], X[:,1], c = preds_eig)
```

![png](/images/output_39_1.png)


## Part H

Next let's experiment with datasets of different noise levels. 


```python
np.random.seed(1234)
n = 1000 #increase sample size to 1000 

#initialize the plot 
fig, ax = plt.subplots(2,3, figsize = (16,8)) 


for i in range(0,3): 
    X, y = datasets.make_moons(n_samples=n, shuffle=True, noise=0.05*(i+1), random_state=None) #generate dataset with different levels of noise
    preds_eig = spectral_clustering(X,epsilon = 0.4) #apply spectral clustering  
    ax[0,i].scatter(X[:,0], X[:,1], c = preds_eig) #plot the fitted labels 
    ax[0,i].set(
        title = f"Noise = {round(0.05*(i+1),2)}, Fitted"
    )
    ax[1,i].scatter(X[:,0], X[:,1], c = y) #plot the actual labels 
    ax[1,i].set(
        title = f"Noise = {round(0.05*(i+1),2)}, Actual"
    )
```


![png](/images/output_41_0.png)


Note that our algorithm is designed to return a 0-1 vector of predicted labels. The way that it decides which group is called 0 and which group is called 1 is arbitrary. This is why colors are flipped in some plots.

Also, we can see that the algorithm starts to have trouble when noise gets to 0.15. The predictions look more like blobs instead of moons


## Part I

Now try our spectral clustering function on another data set -- the bull's eye! 


```python
n = 1000
X, y = datasets.make_circles(n_samples=n, shuffle=True, noise=0.05, random_state=None, factor = 0.4)
plt.scatter(X[:,0], X[:,1])
```

![png](/images/output_44_1.png)


There are two concentric circles. As before k-means will not do well here at all. 


```python
km = KMeans(n_clusters = 2)
km.fit(X)
plt.scatter(X[:,0], X[:,1], c = km.predict(X))
```

![png](/images/output_46_1.png)

This time, let's also experiment with different values of epsilon. 


```python
fig, ax = plt.subplots(1,5, figsize = (16,4))
for i in range(3,8):
    preds = spectral_clustering(X,epsilon = i*0.1)
    ax[i-3].scatter(X[:,0], X[:,1], c = preds)
    ax[i-3].set(title = f'Epsilon = {round(i*0.1,1)}')
```


![png](/images/output_48_0.png)


As shown, our function is able to separate the two rings when 0.4 <= epsilon <= 0.5.

