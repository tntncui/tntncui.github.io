---
layout: post
title: Blog Post 2 - Spectral Clustering
---

In this blog post, I will be showing you how to build a simple spectral clustering algorithm using NumPy and our good old friend Linear Algebra. 


## Introduction 

So what is spectral clustering? As its name mays suggests, it's one method for identifying natural clusters (fancy name for blobs) in a given dataset. So first, let's make a dataset using the make_blobs() function from sklearn. 

```python
import numpy as np
from sklearn import datasets
from matplotlib import pyplot as plt
```


```python
n = 200
np.random.seed(1111)
X, y = datasets.make_blobs(n_samples=n, shuffle=True, random_state=None, centers = 2, cluster_std = 2.0)
plt.scatter(X[:,0], X[:,1])
```

![png](/images/output_2_1.png)

The data we just created has circular blobs. For this type of data, there's another probably more popular method called the K-means clustering. Note in the following plot, K-means successfully identified the two clusters. 


```python
from sklearn.cluster import KMeans
km = KMeans(n_clusters = 2)
km.fit(X)

plt.scatter(X[:,0], X[:,1], c = km.predict(X))
```

![png](/images/output_4_1.png)


But what if natural blobs are not circular? Like if they are shaped as crescents? 


```python
np.random.seed(1234)
n = 200
X, y = datasets.make_moons(n_samples=n, shuffle=True, noise=0.05, random_state=None)
plt.scatter(X[:,0], X[:,1])
```
![png](/images/output_6_1.png)

This time the K-means clustering will yield a poor result. Note in the following plot, K-means attempts to separate the data into clusters by drawing a straight line down the middle. 

```python
km = KMeans(n_clusters = 2)
km.fit(X)
plt.scatter(X[:,0], X[:,1], c = km.predict(X))
```

![png](/images/output_8_1.png)

This is when spectral clustering can be very helpful, as it is not restricted to any particular blob shape. 


## Similarity Matrix  

Our first task is to construct a *similarity matrix* $$\mathbf{A}$$ of size $$n$$x$$n$$ where $$n$$ is the number of observations in our dataset.


When constructing the similarity matrix, we need to specify a parameter `epsilon`, typically between 0 and 1. Entry `A[i,j]` is set to 1 if `X[i]` (the ith observation) is within distance `epsilon` of `X[j]` (the jth observation). Intuitively, we are saying that we want `A[i,j]` to be 1 if `X[i]` is close to `X[j]` and 0 otherwise. 

In addition, the diagonal of A should be all zeros since we don't want to compare a point to itself. In this example, we use `epsilon = 0.4`. 


```python
from sklearn.metrics import pairwise_distances #this function automatically calculates the pairwise distances and stores it in a matrix 

def similarity_matrix(X, epsilon):
    A = pairwise_distances(X)
    A = (A < epsilon).astype(int) #check if each entry is within the designated epsilon
    np.fill_diagonal(A, 0) #fill the diagonal with 0 
    return A 
A = similarity_matrix(X, epsilon = 0.4)
A
```




    array([[0, 0, 0, ..., 0, 0, 0],
           [0, 0, 0, ..., 0, 0, 0],
           [0, 0, 0, ..., 0, 1, 0],
           ...,
           [0, 0, 0, ..., 0, 1, 1],
           [0, 0, 1, ..., 1, 0, 1],
           [0, 0, 0, ..., 1, 1, 0]])

{::options parse_block_html="true" /}
<div class="got-help">
I took the advice from my peers to put these codes into a function, which saves me a few lines when we synthesize everything.
</div>
{::options parse_block_html="false" /}


## Binary Normcut Objective   

One way to view our clustering task is to find a set of labels that minimizes the *binary norm cut*, which is defined as follows: 

$$N_{\mathbf{A}}(C_0, C_1)\equiv \mathbf{cut}(C_0, C_1)\left(\frac{1}{\mathbf{vol}(C_0)} + \frac{1}{\mathbf{vol}(C_1)}\right)\;.$$

In this expression, 
- $$\mathbf{cut}(C_0, C_1) \equiv \sum_{i \in C_0, j \in C_1} A_{ij}$$ is the *cut* of the clusters $$C_0$$ and $$C_1$$. The *cut* between the two clusters can be interpreted as how close $$C_0$$ and $$C_1$$ are to each other
- $$\mathbf{vol}(C_0) \equiv \sum_{i \in C_0}d_i$$, where $$d_i = \sum_{j = 1}^n a_{ij}$$ is the *degree* of row $$i$$ (the total number of all other rows related to row $$i$$ through A). The *volume* of cluster $$C_0$$ is a measure of the size of the cluster. 

This may seem scary, but it won't be so scary as we dissect each term and look at their interpretation. 


#### The Cut Term

By definition, the cut term is the sum of all the nonzero entries `A[i,j]` such that `y[i]` and `y[j]` are in different clusters. And remember when we constructed `A`, we said that `A[i,j]` is nonzero ONLY when `X[i]` and `X[j]` are sufficiently close to each other. Intuitively, a smaller cut term corresponds to further-apart clusters. 

Now let's construct a function called `cut(A,y)` and compute the cut term for the true label vector `y`. 

```python
def cut(A,y):
    res = 0
    for i in range(len(A)): #loop through each entry possible combination 
        for j in range(len(A)):
            if y[i] != y[j]: 
                res += A[i,j] #update res iff the two points are in different clusters
    return res
cut(A,y) #implement the function on the true label vector
```




    26



Let's now create a vector of randomly generated labels and compute its cut term. Note this is a lot larger than the cut term for the true label. This shows that the cut term indeed favors the true clusters over the random ones. 


```python
test = np.random.randint(0,2,size = n) #generate a vector of random labels of size n with each label equal to either 0 or 1
cut(A, test) 
```




    2232



#### The Volume Term 

The next component of the binary norm cut is the volume term. By definition, $$\mathbf{vol}(C_0)$$ is the sum of all the rows i such that observation i belongs in $$C_0$$. Therefore, a bigger volume term corresponds to a bigger cluster. 
Since it's in the denominator, the bigger it is, the smaller the norm cut. 

Now let's write a function `vols(A,y)` that returns `vol(C0)` and `vol(C1)` as a tuple. 


```python
def vols(A,y):
    return np.sum(A[y==0], axis = None), np.sum(A[y==1], axis = None)
vols(A,y)
```




    (2299, 2217)

Finally let's put the pieces together to compute the norm cut for the true label vector `y`. 

```python
def normcut(A,y):
    v0, v1 = vols(A,y)
    return cut(A,y)*(1/v0 + 1/v1)
normcut(A,y)    
```




    0.02303682466323045



Please note that the normcut for the true labels is much smaller than that of the fake labels. This is again a sign that the binary norm cut objective favors the true clusters. 


```python
normcut(A,test)
```




    1.991672673470162



## Some Linear Algebra Tricks  

The binary norm cut objective turns the clustering task into an optimization problem. Unfortunately, the complexity of this approach increases exponentially with $$n$$, which makes it impractical. 

This is when some linear algebra tricks can be very helpful. We need to define a vector $$\mathbf{z} \in \mathbb{R}^n$$ such that: 

$$
z_i = 
\begin{cases}
    \frac{1}{\mathbf{vol}(C_0)} &\quad \text{if } y_i = 0 \\ 
    -\frac{1}{\mathbf{vol}(C_1)} &\quad \text{if } y_i = 1 \\ 
\end{cases}
$$


The values of the entries in $$\mathbf{z}$$ aren't so informative, it's the *signs* that matters. In particular, the $$i$$th observation belongs to $$C_0$$ when $$z_i > 0$$ and $$C_1$$ otherwise. 

This can be connected to the binary norm cut by the following equation. 

$$\mathbf{N}_{\mathbf{A}}(C_0, C_1) = 2\frac{\mathbf{z}^T (\mathbf{D} - \mathbf{A})\mathbf{z}}{\mathbf{z}^T\mathbf{D}\mathbf{z}}\;,$$

where $$\mathbf{D}$$ is the diagonal matrix with nonzero entries $$d_{ii} = d_i$$, and  where $$d_i = \sum_{j = 1}^n a_i$$ is the degree (row-sum) from before.  

First, let's Write a function called `transform(A,y)` to compute the appropriate $$\mathbf{z}$$ vector given `A` and `y`, using the formula above. 

```python
def transform(A,y):
    z = np.zeros(len(y)) #initialize z to be the same length as y 
    v0, v1 = vols(A,y)
    z[y == 0] = 1/v0
    z[y == 1] = 1/v1
    return z
```


Next let's check $$\mathbf{z}^T\mathbf{D}\mathbb{1} = 0$$, where $$\mathbb{1}$$ is the vector of `n` ones (i.e. `np.ones(n)`). Here we are using the `isclose(x,y)` function, which will return True if and only if the given x and y are equal to each other. 

```python
z = transform(A,y)
D = np.diag(sum(A)) #put the row sums on the diagonal of D
D
```




    array([[15.,  0.,  0., ...,  0.,  0.,  0.],
           [ 0., 25.,  0., ...,  0.,  0.,  0.],
           [ 0.,  0., 24., ...,  0.,  0.,  0.],
           ...,
           [ 0.,  0.,  0., ..., 26.,  0.,  0.],
           [ 0.,  0.,  0., ...,  0., 28.,  0.],
           [ 0.,  0.,  0., ...,  0.,  0., 26.]])




```python
np.isclose(z@D@np.ones(n), 0) #check if z@D@I is indeed 0 
```




    True


Finally, let's check the equation above that relates the matrix product to the normcut objective. Again, we'll be using `np.isclose()` function. 


```python
np.isclose(2*(z@(D-A)@z)/(z@D@z), normcut(A,y)) #check if lhs equals to the rhs
```




    True



## Orthogonal Objective 

In the last part, we've shown that the normcut objective is two times the function $$ R_\mathbf{A}(\mathbf{z})$$, where

$$ R_\mathbf{A}(\mathbf{z})\equiv \frac{\mathbf{z}^T (\mathbf{D} - \mathbf{A})\mathbf{z}}{\mathbf{z}^T\mathbf{D}\mathbf{z}} $$

To minimize the normcut objective is thus equivalent to minimize $$ R_\mathbf{A}(\mathbf{z})$$ subject to the condition $$\mathbf{z}^T\mathbf{D}\mathbb{1} = 0$$. 


This can be done so by substituting $$\mathbf{z}$$ for the orthogonal complement of $$\mathbf{z}$$ relative to $$\mathbf{D}\mathbb{1}$$ and then applying the `minimize` function from `scipy.optimize`.


```python
def orth(u, v):
    return (u @ v) / (v @ v)*v

e = np.ones(n) 

d = D @ e

def orth_obj(z):
    z_o = z - orth(z, d)
    return (z_o @ (D - A) @ z_o)/(z_o @ D @ z_o)
```


```python
from scipy.optimize import minimize
z_ = minimize(orth_obj, x0=np.ones(n))
```
 

`z_` is a class containing several objects. The vector that we are looking for is stored at `z_.x`, which we'll name `z_min`. Remember, it's not the values of `z_min` that matters, but rather the signs of z_min. In particular, the $$i$$th observation belongs to $$C_0$$ when `z_min[i] > 0` and $$C_1$$ otherwise. Now let's plot the result. 


```python
z_min = z_.x
preds = np.zeros(len(z_min)) #initialize all entries in preds as zeros
preds[z_min < 0] = 1  #substituting entries where z_min is smaller than 0 
plt.scatter(X[:,0], X[:,1], c = preds)
```

![png](/images/output_31_1.png)


## Laplacian Matrix

While the orthogonal objective seems to be doing a good job, it can get extremely slow as the number of observations increase. After all, the orthogonal objective is basically just the linear algebra version of the normcut objective. What was an optimization problem remains an optimization problem. 


So did we just go through all the linear algebra tricks for vain? No!

Now that we are familiar with setup the diagonal matrix $$\mathbf{D}$$ and the vector $$\mathbf{z}$$, we can introduce the Laplacian Matrix $$\mathbf{L} = \mathbf{D}^{-1}(\mathbf{D} - \mathbf{A})$$. 

Why is this helpful? It turns out that the vector $$\mathbf{z}$$ that we want is simply the eigenvector corresponding to the second smallest eigenvalue of $$\mathbf{L}$$!

```python
#construct the Laplacian matrix 
L = np.linalg.inv(D)@(D-A) 
```


```python
#calculate the eigenvalues and eigenvectors
Lam, U = np.linalg.eig(L)

#sort the eigenvalues and eigenvectors 
ix = Lam.argsort()
Lam, U = Lam[ix], U[:,ix]

#let z_eig be the eigenvector corresponding to the 2nd smallest eigenvalue
z_eig = U[:,1]
```


```python
preds_eig = np.zeros(len(z_eig)) #initialize all entries in preds_eig as zeros
preds_eig[z_eig < 0] = 1  #substituting entries where z_eig is smaller than 0 
plt.scatter(X[:,0], X[:,1], c = preds_eig)
```

![png](/images/output_35_1.png)

Note that `z_eig` did a great job in identifying the two clusters. 

## Synthesizing Results

Now let's put together all the pieces we have. In particular, we'll write a function called `spectral_clustering(X, epsilon)` which takes in the input data `X` and a scalar `epsilon` and performs spectral clustering. The output of this function would be an array of binary labels indicating whether data point i is in group 0  or group 1.

```python
from sklearn.metrics import pairwise_distances 

def spectral_clustering(X,epsilon):
    '''
    Arguments:
    	X = A input data matrix 
    	epsilon = A scalar that measures how close you want the two clusters to be 
    
    Return:
    	A vector of binary labels 
    '''
    #construct similarity matrix A
    A = similarity_matrix(X, epsilon)
    
    #construct the Laplacian matrix
    D = np.diag(sum(A)) #put the row sums on the diagonal of D
    L = np.linalg.inv(D)@(D-A) 
    
    #Compute the eigenvector with second-smallest eigenvalue of the Laplacian matrix
    Lam, U = np.linalg.eig(L)

    #sort the eigenvalues and eigenvectors 
    ix = Lam.argsort()
    Lam, U = Lam[ix], U[:,ix]

    #let z_eig be the eigenvector corresponding to the 2nd smallest eigenvalue
    z_eig = U[:,1]
    
    #form the labels based on the signs of z_eig 
    preds_eig = np.zeros(len(z_eig)) #initialize all entries in preds_eig as zeros
    preds_eig[z_eig < 0] = 1
    
    return preds_eig
```


```python
#plot the result, should be the same to the previous graph 
preds_eig = spectral_clustering(X,epsilon = 0.4)
plt.scatter(X[:,0], X[:,1], c = preds_eig)
```

![png](/images/output_39_1.png)


## Experimenting with Different Noise Levels

Let's put our function into test and experiment with different noise levels. 


```python
np.random.seed(1234)
n = 1000 #increase sample size to 1000 

#initialize the plot 
fig, ax = plt.subplots(2,3, figsize = (16,8)) 


for i in range(0,3): 
    X, y = datasets.make_moons(n_samples=n, shuffle=True, noise=0.05*(i+1), random_state=None) #generate dataset with different levels of noise
    preds_eig = spectral_clustering(X,epsilon = 0.4) #apply spectral clustering  
    ax[0,i].scatter(X[:,0], X[:,1], c = preds_eig) #plot the fitted labels 
    ax[0,i].set(
        title = f"Noise = {round(0.05*(i+1),2)}, Fitted"
    )
    ax[1,i].scatter(X[:,0], X[:,1], c = y) #plot the actual labels 
    ax[1,i].set(
        title = f"Noise = {round(0.05*(i+1),2)}, Actual"
    )
```


![png](/images/output_41_0.png)


Note that our algorithm is designed to return a 0-1 vector of predicted labels. The way that it decides which group is called 0 and which group is called 1 is arbitrary. This is why colors are flipped in some plots.

Also, we can see that the algorithm starts to have trouble when noise gets to 0.15. The predictions look more like blobs instead of moons

{::options parse_block_html="true" /}
<div class="gave-help">
I think putting the experimental results in one graph helps the reader to visualize, so I recommended this to my peers. 
</div>
{::options parse_block_html="false" /}


## Experimenting with Different Dataset and Different Epsilon Values

Now let's try our function on another type of dataset -- the bull's eye. 


```python
n = 1000
X, y = datasets.make_circles(n_samples=n, shuffle=True, noise=0.05, random_state=None, factor = 0.4)
plt.scatter(X[:,0], X[:,1])
```

![png](/images/output_44_1.png)


Since the blobs aren't circular (well, technically they are circular but they are arranged in more like a ring-shape), K-means will fail miserably. 


```python
km = KMeans(n_clusters = 2)
km.fit(X)
plt.scatter(X[:,0], X[:,1], c = km.predict(X))
```

![png](/images/output_46_1.png)

Now let's try spectral clustering. This time, let's also experiment with different values of epsilon. 


```python
fig, ax = plt.subplots(1,5, figsize = (16,4))
for i in range(3,8):
    preds = spectral_clustering(X,epsilon = i*0.1)
    ax[i-3].scatter(X[:,0], X[:,1], c = preds)
    ax[i-3].set(title = f'Epsilon = {round(i*0.1,1)}')
```


![png](/images/output_48_0.png)


As shown, our function is able to separate the two rings when 0.4 <= epsilon <= 0.5.

